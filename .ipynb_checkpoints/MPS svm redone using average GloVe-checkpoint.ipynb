{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "from load_GloVe import *\n",
    "from glove2word2vec import *\n",
    "import os, math, random, pymysql, time, csv, numpy\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # # Convert GloVe to word2vec\n",
    "# # glove_50d = \"glove.6B.50d.txt\"\n",
    "# w2v_50d = \"word2vec.6B.50d.txt\"\n",
    "# # w2v = glove2word2vec(glove_50d, w2v_50d, True) # create w2v file\n",
    "\n",
    "# w2v = Word2Vec.load_word2vec_format(w2v_50d, binary=False)\n",
    "\n",
    "# try: # try to shutdown logging after importing the files\n",
    "#     logging.shutdown()\n",
    "#     print(\"Logging is shut down after glove2word2vec module finish its task\")\n",
    "# except:\n",
    "#     print(\"Logging isn't activated in glove2word2vec module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main category predictor redone in word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Loading data\n",
    "# from helper import *\n",
    "# import time, csv, json, math, random\n",
    "# path_data = '/home/jl/Projects/mps_project/training_data/new_cates.csv'\n",
    "# data = load_csv(path_data, has_header = True)\n",
    "\n",
    "# # Remove stop words\n",
    "# from nltk.corpus import stopwords\n",
    "# unwanted = set([str(word) for word in stopwords.words(\"english\")])\n",
    "# ts = time.time()\n",
    "# for d in data:\n",
    "#     d[0] = \" \".join([word.lower() for word in d[0].split()\\\n",
    "#             if str(word).lower() not in unwanted])\n",
    "#     d[1] = \" \".join([word.lower() for word in d[1].split()\\\n",
    "#             if word.lower() not in unwanted])\n",
    "    \n",
    "# # Use average of word vectors to replace orginal x inputs\n",
    "# err_count = 0\n",
    "# err_messages = {}\n",
    "# data_v = []\n",
    "# for i, d in enumerate(data):\n",
    "#     try:\n",
    "#         x = sent2vec(d[0]*3 + \" \" + d[1], w2v)\n",
    "#         y = d[4]\n",
    "#         data_v.append([x, y])\n",
    "#     except Exception as e:\n",
    "#         err_count += 1\n",
    "#         err_messages[i] = e.message\n",
    "    \n",
    "# # Train test split\n",
    "# train_portion = 0.7\n",
    "# random.seed(2016)\n",
    "# train_ids = set(random.sample([i for i in range(len(data))],\\\n",
    "#                 int(train_portion*len(data)))) # Use a set\n",
    "# train_X, train_Y, test_X, test_Y = [], [], [], []\n",
    "# for i, d in enumerate(data_v):\n",
    "#     if i in train_ids:\n",
    "#         train_x = d[0]\n",
    "#         train_y = d[1]\n",
    "#         train_X.append(train_x)\n",
    "#         train_Y.append(train_y)\n",
    "#     else:\n",
    "#         test_x = d[0]\n",
    "#         test_y = d[1]\n",
    "#         test_X.append(test_x)\n",
    "#         test_Y.append(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Somehow X will contain nan, need to remove\n",
    "# temp_train_X, temp_train_Y = [], []\n",
    "# for i in range(len(train_X)):\n",
    "#     if type(train_X[i]) == numpy.ndarray\\\n",
    "#     and type(train_Y[i]) == str:\n",
    "#         temp_train_X.append(train_X[i])\n",
    "#         temp_train_Y.append(train_Y[i])\n",
    "# train_X = temp_train_X\n",
    "# train_Y = temp_train_Y\n",
    "\n",
    "# temp_test_X, temp_test_Y = [], []\n",
    "# for i in range(len(test_X)):\n",
    "#     if type(test_X[i]) == numpy.ndarray\\\n",
    "#     and type(test_Y[i]) == str:\n",
    "#         temp_test_X.append(test_X[i])\n",
    "#         temp_test_Y.append(test_Y[i])\n",
    "# test_X = temp_test_X\n",
    "# test_Y = temp_test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning takes: 10781.97s\n",
      "Testing takes: 0.15s\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "         Animals & Pet Supplies       0.67      0.71      0.69       678\n",
      "            Apparel Accessories       0.46      0.25      0.33      1431\n",
      "           Arts & Entertainment       0.64      0.34      0.45       412\n",
      "                   Auto & Tires       0.50      0.49      0.50      2669\n",
      "                 Baby & Toddler       0.54      0.53      0.53      2085\n",
      "          Business & Industrial       0.00      0.00      0.00       365\n",
      "                       Clothing       0.59      0.88      0.70      2544\n",
      "                    Electronics       0.59      0.71      0.65      2017\n",
      "                   Female Shoes       0.62      0.37      0.46       680\n",
      "               Food & Beverages       0.68      0.67      0.68       507\n",
      "                       Hardware       0.00      0.00      0.00       502\n",
      "                Health & Beauty       0.60      0.70      0.64      1870\n",
      "                  Home & Garden       0.49      0.50      0.50      2352\n",
      "                Home Apppiances       0.54      0.73      0.62      4334\n",
      "                        Jewelry       0.66      0.86      0.74       746\n",
      "                 Luggage & Bags       0.69      0.56      0.62       672\n",
      "                     Male Shoes       0.65      0.31      0.42       644\n",
      "                         Mature       0.67      0.01      0.02       247\n",
      "                          Media       0.67      0.65      0.66       275\n",
      "                   Photo Center       0.61      0.44      0.51      1077\n",
      "         Religious & Ceremonial       0.72      0.36      0.48       303\n",
      "                       Software       0.61      0.69      0.65        95\n",
      "                 Sporting Goods       1.00      0.00      0.00       405\n",
      "Tobacco & Electronic Cigarettes       0.82      0.04      0.08       209\n",
      "                   Toys & Games       0.53      0.50      0.51       634\n",
      "\n",
      "                    avg / total       0.56      0.57      0.54     27753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Training\n",
    "# from sklearn.svm import LinearSVC\n",
    "# ts = time.time()\n",
    "# clf = LinearSVC()\n",
    "# clf.fit(train_X, train_Y)\n",
    "# t1 = time.time()\n",
    "# print(\"Traning takes: {}s\".format(round(t1-ts, 2)))\n",
    "\n",
    "# # Test\n",
    "# from sklearn.metrics import classification_report\n",
    "# Y_pred = clf.predict(test_X)\n",
    "# t2 = time.time()\n",
    "# print(\"Testing takes: {}s\".format(round(t2-t1, 2)))\n",
    "# print(classification_report(test_Y, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 dimension doesn't seem to be a good enough dimension. Let's see how would double the dimension help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use 100-d word vectors to redo the svm classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-12-01 21:15:27,786 : MainThread : INFO : loading projection weights from /home/jl/Projects/play word embeddings/word_vectors/word2vec.6B.100d.txt\n",
      "2016-12-01 21:19:04,038 : MainThread : INFO : loaded (400000, 100) matrix from /home/jl/Projects/play word embeddings/word_vectors/word2vec.6B.100d.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging is shut down after glove2word2vec module finish its task\n"
     ]
    }
   ],
   "source": [
    "# Convert GloVe to word2vec\n",
    "# glove_100d = '/home/jl/Projects/play word embeddings/word_vectors/glove.6B.100d.txt'\n",
    "w2v_100d = '/home/jl/Projects/play word embeddings/word_vectors/word2vec.6B.100d.txt'\n",
    "# glove2word2vec(glove_100d, w2v_100d, False) # create w2v file\n",
    "\n",
    "w2v = Word2Vec.load_word2vec_format(w2v_100d, binary=False)\n",
    "\n",
    "try: # try to shutdown logging after importing the files\n",
    "    logging.shutdown()\n",
    "    print(\"Logging is shut down after glove2word2vec module finish its task\")\n",
    "except:\n",
    "    print(\"Logging isn't activated in glove2word2vec module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "from helper import *\n",
    "import time, csv, json, math, random\n",
    "path_data = '/home/jl/Projects/mps_project/training_data/new_cates.csv'\n",
    "data = load_csv(path_data, has_header = True)\n",
    "\n",
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "unwanted = set([str(word) for word in stopwords.words(\"english\")])\n",
    "ts = time.time()\n",
    "for d in data:\n",
    "    d[0] = \" \".join([word.lower() for word in d[0].split()\\\n",
    "            if str(word).lower() not in unwanted])\n",
    "    d[1] = \" \".join([word.lower() for word in d[1].split()\\\n",
    "            if word.lower() not in unwanted])\n",
    "\n",
    "# Use average of word vectors to replace orginal x inputs\n",
    "err_count = 0\n",
    "err_messages = {}\n",
    "data_v = []\n",
    "for i, d in enumerate(data):\n",
    "    try:\n",
    "        x = sent2vec(d[0]*3 + \" \" + d[1], w2v)\n",
    "        y = d[4]\n",
    "        data_v.append([x, y])\n",
    "    except Exception as e:\n",
    "        err_count += 1\n",
    "        err_messages[i] = e.message\n",
    "    \n",
    "# Train test split\n",
    "train_portion = 0.7\n",
    "random.seed(2016)\n",
    "train_ids = set(random.sample([i for i in range(len(data))],\\\n",
    "                int(train_portion*len(data)))) # Use a set\n",
    "train_X, train_Y, test_X, test_Y = [], [], [], []\n",
    "for i, d in enumerate(data_v):\n",
    "    if i in train_ids:\n",
    "        train_x = d[0]\n",
    "        train_y = d[1]\n",
    "        train_X.append(train_x)\n",
    "        train_Y.append(train_y)\n",
    "    else:\n",
    "        test_x = d[0]\n",
    "        test_y = d[1]\n",
    "        test_X.append(test_x)\n",
    "        test_Y.append(test_y)\n",
    "\n",
    "# Somehow X will contain nan, need to remove\n",
    "temp_train_X, temp_train_Y = [], []\n",
    "for i in range(len(train_X)):\n",
    "    if type(train_X[i]) == numpy.ndarray\\\n",
    "    and type(train_Y[i]) == str:\n",
    "        temp_train_X.append(train_X[i])\n",
    "        temp_train_Y.append(train_Y[i])\n",
    "train_X = temp_train_X\n",
    "train_Y = temp_train_Y\n",
    "\n",
    "temp_test_X, temp_test_Y = [], []\n",
    "for i in range(len(test_X)):\n",
    "    if type(test_X[i]) == numpy.ndarray\\\n",
    "    and type(test_Y[i]) == str:\n",
    "        temp_test_X.append(test_X[i])\n",
    "        temp_test_Y.append(test_Y[i])\n",
    "test_X = temp_test_X\n",
    "test_Y = temp_test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning takes: 557.88s\n",
      "Testing takes: 0.16s\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "         Animals & Pet Supplies       0.75      0.74      0.74       678\n",
      "            Apparel Accessories       0.64      0.41      0.50      1431\n",
      "           Arts & Entertainment       0.70      0.52      0.60       412\n",
      "                   Auto & Tires       0.57      0.60      0.59      2669\n",
      "                 Baby & Toddler       0.64      0.65      0.64      2085\n",
      "          Business & Industrial       0.88      0.15      0.26       365\n",
      "                       Clothing       0.71      0.90      0.80      2544\n",
      "                    Electronics       0.67      0.75      0.71      2017\n",
      "                   Female Shoes       0.75      0.62      0.68       680\n",
      "               Food & Beverages       0.76      0.73      0.74       507\n",
      "                       Hardware       0.50      0.02      0.03       502\n",
      "                Health & Beauty       0.70      0.79      0.74      1870\n",
      "                  Home & Garden       0.57      0.58      0.57      2352\n",
      "                Home Apppiances       0.63      0.77      0.69      4334\n",
      "                        Jewelry       0.77      0.90      0.83       746\n",
      "                 Luggage & Bags       0.70      0.75      0.72       672\n",
      "                     Male Shoes       0.66      0.62      0.64       644\n",
      "                         Mature       0.77      0.15      0.25       247\n",
      "                          Media       0.70      0.72      0.71       275\n",
      "                   Photo Center       0.71      0.60      0.65      1077\n",
      "         Religious & Ceremonial       0.76      0.54      0.63       303\n",
      "                       Software       0.84      0.85      0.85        95\n",
      "                 Sporting Goods       0.67      0.10      0.18       405\n",
      "Tobacco & Electronic Cigarettes       0.91      0.29      0.44       209\n",
      "                   Toys & Games       0.59      0.55      0.57       634\n",
      "\n",
      "                    avg / total       0.66      0.66      0.64     27753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from sklearn.svm import LinearSVC\n",
    "ts = time.time()\n",
    "clf = LinearSVC()\n",
    "clf.fit(train_X, train_Y)\n",
    "t1 = time.time()\n",
    "print(\"Traning takes: {}s\".format(round(t1-ts, 2)))\n",
    "\n",
    "# Test\n",
    "from sklearn.metrics import classification_report\n",
    "Y_pred = clf.predict(test_X)\n",
    "t2 = time.time()\n",
    "print(\"Testing takes: {}s\".format(round(t2-t1, 2)))\n",
    "print(classification_report(test_Y, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better but doesn't seem to be better than bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
